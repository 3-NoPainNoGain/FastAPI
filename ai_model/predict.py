import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import torch.optim as optim

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"GPU ì‚¬ìš© ì—¬ë¶€: {'ì‚¬ìš©' if torch.cuda.is_available() else 'ë¯¸ì‚¬ìš©'}")

# ë°ì´í„° ê²½ë¡œ ì„¤ì • (VS Code ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
BASE_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(BASE_DIR, "datasets", "processed")

# ë°ì´í„° ë¡œë“œ
X_orig = np.load(os.path.join(DATA_DIR, "X.npy"))
Y_orig = np.load(os.path.join(DATA_DIR, "Y.npy"))
X_aug = np.load(os.path.join(DATA_DIR, "augmented_X.npy"))
Y_aug = np.load(os.path.join(DATA_DIR, "augmented_Y.npy"))

# âœ… í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜ í™•ì¸ ìœ„ì¹˜ 
print("ì›ë³¸ í´ë˜ìŠ¤ ëª©ë¡:", set(np.unique(Y_orig)))
print("ì¦ê°• í´ë˜ìŠ¤ ëª©ë¡:", set(np.unique(Y_aug)))

if set(np.unique(Y_orig)) != set(np.unique(Y_aug)):
    print("âš ï¸ í´ë˜ìŠ¤ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤! ë³‘í•©í•˜ê¸° ì „ì— ì¦ê°• ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
    exit()  # ë˜ëŠ” raise ValueError("í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜!")

# í´ë˜ìŠ¤ ë³‘í•©
X = np.concatenate([X_orig, X_aug], axis=0)
Y = np.concatenate([Y_orig, Y_aug], axis=0)
print(f"ì´ ë°ì´í„° shape: {X.shape}, ì´ ë¼ë²¨ shape: {Y.shape}")

# ì •ê·œí™”
scaler = StandardScaler()
X = X.reshape(-1, 258)
X = scaler.fit_transform(X)
X = X.reshape(-1, 30, 258)

# Train/Validation Split
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

# í…ì„œ ë³€í™˜
X_train = torch.tensor(X_train, dtype=torch.float32)
Y_train = torch.tensor(Y_train, dtype=torch.long)
X_val = torch.tensor(X_val, dtype=torch.float32)
Y_val = torch.tensor(Y_val, dtype=torch.long)

# ë°ì´í„°ë¡œë”
batch_size = 16
train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batch_size, shuffle=False)

# ëª¨ë¸ ì •ì˜
class SignLanguage1DCNN(nn.Module):
    def __init__(self, num_classes):
        super(SignLanguage1DCNN, self).__init__()
        self.conv1 = nn.Conv1d(258, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool = nn.MaxPool1d(2)
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(128 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.activation = nn.LeakyReLU()

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.pool(self.activation(self.bn1(self.conv1(x))))
        x = self.pool(self.activation(self.bn2(self.conv2(x))))
        x = x.view(x.size(0), -1)
        x = self.dropout(self.activation(self.fc1(x)))
        return self.fc2(x)
    
    # ğŸ‘‡ load_model í•¨ìˆ˜ ì •ì˜
    def load_model(model_path=None):
        BASE_DIR = os.path.dirname(__file__)
        DATA_DIR = os.path.join(BASE_DIR, "datasets", "processed")
        MODEL_PATH = model_path or os.path.join(BASE_DIR, "models", "aug_model_val_99.17_20250525.pth")

        classes = np.load(os.path.join(DATA_DIR, "classes.npy"), allow_pickle=True)
        model = SignLanguage1DCNN(num_classes=len(classes))
        model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device("cpu")))
        model.eval()
        return model

# í•™ìŠµ ì¤€ë¹„
model = SignLanguage1DCNN(num_classes=len(np.unique(Y))).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_loader, val_loader, epochs=30):
    best_accuracy = 0.0
    early_stop_count = 0
    patience = 10
    
    for epoch in range(epochs):
        model.train()
        train_loss, correct_train, total_train = 0.0, 0, 0

        for X_batch, Y_batch in train_loader:
            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, Y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            correct_train += (outputs.argmax(1) == Y_batch).sum().item()
            total_train += Y_batch.size(0)

        train_accuracy = 100 * correct_train / total_train

        # ê²€ì¦
        model.eval()
        val_loss, correct_val, total_val = 0.0, 0, 0
        with torch.no_grad():
            for X_batch, Y_batch in val_loader:
                X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
                outputs = model(X_batch)
                val_loss += criterion(outputs, Y_batch).item()
                correct_val += (outputs.argmax(1) == Y_batch).sum().item()
                total_val += Y_batch.size(0)

        val_accuracy = 100 * correct_val / total_val
        print(f"[Epoch {epoch+1}/{epochs}] Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%")

        scheduler.step()

        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), os.path.join(DATA_DIR, "aug_model_val_99.17_20250525.pth"))
            print("â­ Best model saved")
            early_stop_count = 0
        else:
            early_stop_count += 1
            if early_stop_count >= patience:
                print("â›” Early stopping")
                break

# í•™ìŠµ ì‹œì‘
train_model(model, train_loader, val_loader)

# ìµœì¢… ì •í™•ë„ ì¶œë ¥
model.load_state_dict(torch.load(os.path.join(BASE_DIR, "models", "aug_model_val_99.17_20250525.pth")))
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for X_batch, Y_batch in val_loader:
        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
        outputs = model(X_batch)
        correct += (outputs.argmax(1) == Y_batch).sum().item()
        total += Y_batch.size(0)

print(f"ìµœì¢… Validation Accuracy: {100 * correct / total:.2f}%")

# í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ
CLASSES_PATH = os.path.join(DATA_DIR, "classes.npy")
if os.path.exists(CLASSES_PATH):
    classes = np.load(CLASSES_PATH, allow_pickle=True)
else:
    raise FileNotFoundError("classes.npy íŒŒì¼ì´ datasets/processed/ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤.")

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict_from_keypoints(keypoints_30x258, model=None):
    if model is None:
        model = SignLanguage1DCNN(num_classes=len(classes))
        model.load_state_dict(torch.load(os.path.join(DATA_DIR, "aug_model_val_99.17_20250525.pth"), map_location=device))
        model.eval()

    if isinstance(keypoints_30x258, list):
        keypoints_30x258 = np.array(keypoints_30x258)

    if keypoints_30x258.shape != (30, 258):
        raise ValueError(f"â— ì…ë ¥ shape ì˜¤ë¥˜: ê¸°ëŒ€ê°’ì€ (30, 258), í˜„ì¬ëŠ” {keypoints_30x258.shape}")

    input_tensor = torch.tensor(keypoints_30x258, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(input_tensor)
        pred_idx = output.argmax(dim=1).item()
        return classes[pred_idx]
    
    # ğŸ‘‡ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_model(model_path=None):
    BASE_DIR = os.path.dirname(__file__)
    DATA_DIR = os.path.join(BASE_DIR, "datasets", "processed")
    MODEL_PATH = model_path or os.path.join(BASE_DIR, "models", "aug_model_val_99.17_20250525.pth")

    classes = np.load(os.path.join(DATA_DIR, "classes.npy"), allow_pickle=True)

    model = SignLanguage1DCNN(num_classes=len(classes))
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device("cpu")))
    model.eval()
    return model


# ğŸ‘‡ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_from_keypoints(keypoints_30x258, model=None):
    if model is None:
        model = load_model()

    if isinstance(keypoints_30x258, list):
        keypoints_30x258 = np.array(keypoints_30x258)

    if keypoints_30x258.shape != (30, 258):
        raise ValueError(f"â— ì…ë ¥ shape ì˜¤ë¥˜: ê¸°ëŒ€ê°’ì€ (30, 258), í˜„ì¬ëŠ” {keypoints_30x258.shape}")

    input_tensor = torch.tensor(keypoints_30x258, dtype=torch.float32).unsqueeze(0)  # (1, 30, 258)

    with torch.no_grad():
        output = model(input_tensor)
        pred_idx = output.argmax(dim=1).item()

        classes = np.load(os.path.join(os.path.dirname(__file__), "datasets", "processed", "classes.npy"), allow_pickle=True)
        return classes[pred_idx]
